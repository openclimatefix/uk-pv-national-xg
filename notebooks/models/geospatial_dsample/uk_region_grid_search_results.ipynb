{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load locally saved results of gridsearch\n",
    "results = pd.read_pickle(\"/home/tom/local_data/xgboost_uk_region_grid_search_results.p\")\n",
    "best_params = results.sort_values(\"NegMAE\", ascending=False).iloc[0]  # best params\n",
    "\n",
    "# recast some params to float\n",
    "best_params.loc[[\"gamma\", \"reg_alpha\", \"reg_lambda\"]] = best_params.loc[[\"gamma\", \"reg_alpha\", \"reg_lambda\"]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "colsample_bytree         0.75\n",
       "gamma                     0.0\n",
       "grow_policy         depthwise\n",
       "learning_rate            0.01\n",
       "max_depth                 300\n",
       "min_child_weight            5\n",
       "n_estimators             2000\n",
       "reg_alpha                 0.0\n",
       "reg_lambda                0.0\n",
       "subsample                0.95\n",
       "NegMAE              -0.021343\n",
       "Name: 866, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each param, do a finer grid search\n",
    "# consider other params fixed during search to reduce the time taken\n",
    "\n",
    "DEFAULT_PERCENTILES = np.asarray([0.75, 0.85, 1.25, 1.4])\n",
    "DEFFAULT_HYPARAM_CONFIG = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"colsample_bylevel\": 1,\n",
    "    \"colsample_bynode\": 1,\n",
    "    \"colsample_bytree\": 0.85,\n",
    "    \"early_stopping_rounds\": None,\n",
    "    \"gamma\": 0,\n",
    "    \"gpu_id\": -1,\n",
    "    \"grow_policy\": \"depthwise\",\n",
    "    \"importance_type\": None,\n",
    "    \"interaction_constraints\": \"\",\n",
    "    \"learning_rate\": 0.005,\n",
    "    \"max_bin\": 256,\n",
    "    \"max_cat_threshold\": 64,\n",
    "    \"max_depth\": 100,\n",
    "    \"max_leaves\": 0,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"n_estimators\": 1_500,\n",
    "    \"n_jobs\": -1,\n",
    "    \"num_parallel_tree\": 1,\n",
    "    \"predictor\": \"auto\",\n",
    "    \"random_state\": 0,\n",
    "    \"reg_alpha\": 0,\n",
    "    \"reg_lambda\": 1,\n",
    "    \"sampling_method\": \"uniform\",\n",
    "    \"scale_pos_weight\": 1,\n",
    "    \"subsample\": 0.65,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"validate_parameters\": 1,\n",
    "    \"verbosity\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "def create_param_grids( current_params: dict[str, float], percentiles:np.ndarray = DEFAULT_PERCENTILES):\n",
    "    grid = list()\n",
    "    for param, param_value in current_params.items():\n",
    "        if isinstance(param_value, str):\n",
    "            pass\n",
    "        else:\n",
    "            if np.isclose(param_value, 0, rtol=1e-6):\n",
    "                grid.append(pd.Series(index=range(len(percentiles)), data =np.linspace(0, 2, len(DEFAULT_PERCENTILES)), name=param))\n",
    "            else:\n",
    "                grid.append(pd.Series(index=range(len(percentiles)), data = DEFAULT_PERCENTILES*param_value, name=param))\n",
    "        \n",
    "    return pd.concat(grid, axis=1)\n",
    "\n",
    "def get_default_hyperparam_config() -> dict:\n",
    "    return DEFFAULT_HYPARAM_CONFIG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data for running experiment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pandas.core.dtypes.common import is_datetime64_dtype\n",
    "import numpy.typing as npt\n",
    "from typing import Union, Tuple, List\n",
    "\n",
    "nwp_path = (\n",
    "    \"gs://solar-pv-nowcasting-data/NWP/UK_Met_Office/UKV_intermediate_version_3.zarr/\"\n",
    ")\n",
    "gsp_path = \"gs://solar-pv-nowcasting-data/PV/GSP/v5/pv_gsp.zarr\"\n",
    "\n",
    "TRIG_DATETIME_FEATURE_NAMES = [\n",
    "    \"SIN_MONTH\",\n",
    "    \"COS_MONTH\",\n",
    "    \"SIN_DAY\",\n",
    "    \"COS_DAY\",\n",
    "    \"SIN_HOUR\",\n",
    "    \"COS_HOUR\",\n",
    "]\n",
    "\n",
    "\n",
    "def _get_path_to_uk_region_data_data(\n",
    "    variable: str, forecast_horizon: int, inner: bool\n",
    ") -> str:\n",
    "    if inner:\n",
    "        return f\"/home/tom/local_data/uk_region_mean_var{variable}_step{forecast_horizon}.npy\"\n",
    "    else:\n",
    "        return f\"/home/tom/local_data/outer_region_mean_var{variable}_step{forecast_horizon}.npy\"\n",
    "\n",
    "\n",
    "def _trig_transform(\n",
    "    values: np.ndarray, period: Union[float, int]\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Given a list of values and an upper limit on the values, compute trig decomposition.\n",
    "    Args:\n",
    "        values: ndarray of points in the range [0, period]\n",
    "        period: period of the data\n",
    "    Returns:\n",
    "        Decomposition of values into sine and cosine of data with given period\n",
    "    \"\"\"\n",
    "\n",
    "    return np.sin(values * 2 * np.pi / period), np.cos(values * 2 * np.pi / period)\n",
    "\n",
    "\n",
    "def trigonometric_datetime_transformation(datetimes: npt.ArrayLike) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Given an iterable of datetimes, returns a trigonometric decomposition on hour, day and month\n",
    "    Args:init_time=30,\n",
    "        datetimes: ArrayLike of datetime64 values\n",
    "    Returns:\n",
    "        Trigonometric decomposition of datetime into hourly, daily and\n",
    "        monthly values.\n",
    "    \"\"\"\n",
    "    assert is_datetime64_dtype(\n",
    "        datetimes\n",
    "    ), \"Data for Trig Decomposition must be np.datetime64 type\"\n",
    "\n",
    "    datetimes = pd.DatetimeIndex(datetimes)\n",
    "    hour = datetimes.hour.values.reshape(-1, 1) + (\n",
    "        datetimes.minute.values.reshape(-1, 1) / 60\n",
    "    )\n",
    "    day = datetimes.day.values.reshape(-1, 1)\n",
    "    month = datetimes.month.values.reshape(-1, 1)\n",
    "\n",
    "    sine_hour, cosine_hour = _trig_transform(hour, 24)\n",
    "    sine_day, cosine_day = _trig_transform(day, 366)\n",
    "    sine_month, cosine_month = _trig_transform(month, 12)\n",
    "\n",
    "    return np.concatenate(\n",
    "        [sine_month, cosine_month, sine_day, cosine_day, sine_hour, cosine_hour], axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "DEFAULT_UK_REGION_NWP_VARS = [\"dswrf\", \"hcct\", \"t\", \"lcc\", \"sde\"]\n",
    "\n",
    "\n",
    "def build_datasets_from_local(\n",
    "        eval_timeseries: pd.DatetimeIndex,\n",
    "        step: int,\n",
    "        variables: list[str] = DEFAULT_UK_REGION_NWP_VARS,\n",
    "        nan_to_zero: bool = False,\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "    X = list()\n",
    "    for var in variables:\n",
    "        X.append(np.load(_get_path_to_uk_region_data_data(var, step, True)))\n",
    "        X.append(np.load(_get_path_to_uk_region_data_data(var, step, False)))\n",
    "    X = np.concatenate(X, axis=0).T\n",
    "    X = np.nan_to_num(X) if nan_to_zero else X\n",
    "\n",
    "    columns = []\n",
    "    for var in variables:\n",
    "        columns += [f\"{var}_{region}\" for region in [\"within\", \"outer\"]]\n",
    "\n",
    "    X = pd.DataFrame(data=X, columns=columns, index=eval_timeseries)\n",
    "    y = pd.DataFrame(\n",
    "        gsp[\"generation_mw\"] / gsp[\"installedcapacity_mwp\"],\n",
    "        index=eval_timeseries,\n",
    "        columns=[\"target\"],\n",
    "    )\n",
    "\n",
    "    # shift y by the step forecast\n",
    "    shift = nwp.step.values[step]\n",
    "    y = y.shift(freq=-shift).dropna()\n",
    "    common_index = sorted(pd.DatetimeIndex((set(y.index).intersection(X.index))))\n",
    "\n",
    "    X, y = X.loc[common_index], y.loc[common_index]\n",
    "\n",
    "    # add datetime methods for the point at which we are forecasting e.g. now + step\n",
    "    _X = trigonometric_datetime_transformation(\n",
    "        y.shift(freq=nwp.step.values[step]).index.values\n",
    "    )\n",
    "    _X = pd.DataFrame(_X, index=y.index, columns=TRIG_DATETIME_FEATURE_NAMES)\n",
    "    X = pd.concat([X, _X], axis=1)\n",
    "\n",
    "    # add lagged values of GSP PV\n",
    "    ar_1 = y.shift(freq=-(shift + np.timedelta64(1, \"h\")))\n",
    "    ar_day = y.shift(freq=-(shift + np.timedelta64(1, \"D\")))\n",
    "    ar_1.columns = [\"PV_LAG_1HR\"]\n",
    "    ar_day.columns = [\"PV_LAG_DAY\"]\n",
    "\n",
    "    # estimate linear trend of the PV\n",
    "    window_size = 10\n",
    "    epsilon = 0.01\n",
    "    y_covariates = y.shift(freq=-(shift + np.timedelta64(2, \"h\")))\n",
    "    y_covariates.columns = [\"x\"]\n",
    "    y_target = y.shift(freq=-(shift + np.timedelta64(1, \"h\")))\n",
    "    y_target.columns = [\"y\"]\n",
    "    data = pd.concat([y_target, y_covariates], axis=1).dropna()\n",
    "    _x = data[\"x\"].values\n",
    "    _y = data[\"y\"].values\n",
    "    _betas = np.nan * np.empty(len(data))\n",
    "\n",
    "    for n in range(window_size, len(data)):\n",
    "        __y = _y[(n - window_size) : n]\n",
    "        __x = _x[(n - window_size) : n]\n",
    "        __b = max(min((1 / ((__x.T @ __x) + epsilon)) * (__x.T @ __y), 10), -10)\n",
    "        _betas[n] = __b\n",
    "\n",
    "    betas = pd.DataFrame(data=_betas, columns=[\"AR_Beta\"], index=data.index)\n",
    "\n",
    "    X = pd.concat([X, ar_1, ar_day, betas], axis=1).dropna()\n",
    "    y = y.loc[X.index]\n",
    "\n",
    "    return X, y\n",
    "    \n",
    "def build_ts_data_cv_splitting(\n",
    "    X: pd.DataFrame, n_splits: int, val_size: int\n",
    ") -> List[Tuple]:\n",
    "    X_tests = range(0, len(X) - val_size, int((len(X) - val_size) / n_splits))\n",
    "    prev_idx = 0\n",
    "    indicies = list()\n",
    "\n",
    "    for idx in X_tests[1:]:\n",
    "        indicies.append((list(range(prev_idx, idx)), list(range(idx, idx + val_size))))\n",
    "\n",
    "    return indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsp = xr.open_zarr(gsp_path)\n",
    "nwp = xr.open_zarr(nwp_path)\n",
    "\n",
    "evaluation_timeseries = (\n",
    "    gsp.coords[\"datetime_gmt\"]\n",
    "    .where(\n",
    "        (gsp[\"datetime_gmt\"] >= nwp.coords[\"init_time\"].values[0])\n",
    "        & (gsp[\"datetime_gmt\"] <= nwp.coords[\"init_time\"].values[-1]),\n",
    "        drop=True,\n",
    "    )\n",
    "    .values\n",
    ")\n",
    "\n",
    "gsp = gsp.sel(datetime_gmt=evaluation_timeseries, gsp_id=0)\n",
    "X, y = build_datasets_from_local(\n",
    "    evaluation_timeseries, 30\n",
    ")  # choose forecast horizon 30 for the CV\n",
    "_cv = build_ts_data_cv_splitting(X, 3, 2_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params.to_dict()[\"n_estimators\"].__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': [0.5625, 0.6375, 0.9375, 1.0],\n",
       " 'gamma': [0.0, 0.6666666666666666, 1.3333333333333333, 2.0],\n",
       " 'learning_rate': [0.0075, 0.0085, 0.0125, 0.013999999999999999],\n",
       " 'max_depth': [225, 255, 375, 420],\n",
       " 'min_child_weight': [3, 4, 6, 7],\n",
       " 'n_estimators': [1500, 1700, 2500, 2800],\n",
       " 'reg_alpha': [0.0, 0.6666666666666666, 1.3333333333333333, 2.0],\n",
       " 'reg_lambda': [0.0, 0.6666666666666666, 1.3333333333333333, 2.0],\n",
       " 'subsample': [0.7124999999999999, 0.8075, 1.0, 1.0]}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best_params.drop(\"NegMAE\", inplace=True)\n",
    "\n",
    "param_search_grid = create_param_grids(best_params.to_dict())\n",
    "param_search_grid[[\"colsample_bytree\", \"subsample\"]] = param_search_grid[[\"colsample_bytree\", \"subsample\"]].clip(0, 1)\n",
    "param_search_grid[[\"max_depth\", \"min_child_weight\", \"n_estimators\"]] = param_search_grid[[\"max_depth\", \"min_child_weight\", \"n_estimators\"]].astype(np.int64)\n",
    "param_search_grid = param_search_grid.to_dict(orient=\"list\")\n",
    "param_search_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "def run_grid_search_experiment(default_xgboost_params, gsearch_params):\n",
    "    gsearch = GridSearchCV(\n",
    "        XGBRegressor(**default_xgboost_params),\n",
    "        param_grid=gsearch_params,\n",
    "        scoring=\"neg_mean_absolute_error\",\n",
    "        verbose=1,\n",
    "        cv=_cv\n",
    "    )\n",
    "    \n",
    "    results = gsearch.fit(X, y)\n",
    "    scores = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(results.cv_results_[\"params\"]),\n",
    "            pd.DataFrame(results.cv_results_[\"mean_test_score\"], columns=[\"NegMAE\"]),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "output = dict()\n",
    "\n",
    "for param in param_search_grid:\n",
    "    default_hyperparams = get_default_hyperparam_config()\n",
    "    del default_hyperparams[param]  # remove param from default parameters\n",
    "    \n",
    "    search = {\n",
    "        param: param_search_grid[param]\n",
    "    }\n",
    "    \n",
    "    output[param] = run_grid_search_experiment(default_hyperparams, search)\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = dict()\n",
    "\n",
    "for param, x in output.items():\n",
    "    optimal_params[param] = x.sort_values(\"NegMAE\", ascending=False).iloc[0][param]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6375,\n",
       " 'gamma': 0.6666666666666666,\n",
       " 'learning_rate': 0.0075,\n",
       " 'max_depth': 225.0,\n",
       " 'min_child_weight': 6.0,\n",
       " 'n_estimators': 2500.0,\n",
       " 'reg_alpha': 0.6666666666666666,\n",
       " 'reg_lambda': 2.0,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xg_pv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d6d38ec70d70ff95971ecacbbcc644fed77284fe18ba773f3d73e7a6117402b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
